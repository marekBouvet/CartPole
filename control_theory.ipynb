{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ebdc81e4-a82a-45a9-82ba-43cc1606baff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import math\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7562526b-8e8b-4343-b3d8-6bf8d63e7548",
   "metadata": {},
   "outputs": [],
   "source": [
    "def control_theory(theta):\n",
    "    # threshold = math.pi / 50\n",
    "    threshold = 0\n",
    "    if theta < threshold:\n",
    "        action = 0\n",
    "    elif theta >= threshold:\n",
    "        action = 1\n",
    "    return action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c38e617-a866-412c-aec0-0f39dcabde01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation space: Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)\n",
      "action space: Discrete(2)\n",
      "threshold:  475.0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1', render_mode=\"human\")\n",
    "print('observation space:', env.observation_space)\n",
    "print('action space:', env.action_space)\n",
    "threshold = env.spec.reward_threshold\n",
    "print('threshold: ', threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "366d84d9-edd1-459b-afa4-2405239f594f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, Total reward: 38.0\n",
      "Episode: 1, Total reward: 57.0\n",
      "Episode: 2, Total reward: 25.0\n",
      "Episode: 3, Total reward: 39.0\n",
      "Episode: 4, Total reward: 42.0\n",
      "Episode: 5, Total reward: 50.0\n",
      "Episode: 6, Total reward: 64.0\n",
      "Episode: 7, Total reward: 48.0\n",
      "Episode: 8, Total reward: 32.0\n",
      "Episode: 9, Total reward: 45.0\n",
      "Episode: 10, Total reward: 56.0\n",
      "Episode: 11, Total reward: 37.0\n",
      "Episode: 12, Total reward: 39.0\n",
      "Episode: 13, Total reward: 52.0\n",
      "Episode: 14, Total reward: 45.0\n",
      "Episode: 15, Total reward: 56.0\n",
      "Episode: 16, Total reward: 37.0\n",
      "Episode: 17, Total reward: 25.0\n",
      "Episode: 18, Total reward: 53.0\n",
      "Episode: 19, Total reward: 32.0\n",
      "Episode: 20, Total reward: 31.0\n",
      "Episode: 21, Total reward: 39.0\n",
      "Episode: 22, Total reward: 39.0\n",
      "Episode: 23, Total reward: 52.0\n",
      "Episode: 24, Total reward: 36.0\n",
      "Episode: 25, Total reward: 35.0\n",
      "Episode: 26, Total reward: 42.0\n",
      "Episode: 27, Total reward: 45.0\n",
      "Episode: 28, Total reward: 52.0\n",
      "Episode: 29, Total reward: 38.0\n",
      "Episode: 30, Total reward: 47.0\n",
      "Episode: 31, Total reward: 34.0\n",
      "Episode: 32, Total reward: 39.0\n",
      "Episode: 33, Total reward: 47.0\n",
      "Episode: 34, Total reward: 52.0\n",
      "Episode: 35, Total reward: 40.0\n",
      "Episode: 36, Total reward: 39.0\n",
      "Episode: 37, Total reward: 36.0\n",
      "Episode: 38, Total reward: 34.0\n",
      "Episode: 39, Total reward: 52.0\n",
      "Episode: 40, Total reward: 44.0\n",
      "Episode: 41, Total reward: 34.0\n",
      "Episode: 42, Total reward: 40.0\n",
      "Episode: 43, Total reward: 46.0\n",
      "Episode: 44, Total reward: 34.0\n",
      "Episode: 45, Total reward: 50.0\n",
      "Episode: 46, Total reward: 26.0\n",
      "Episode: 47, Total reward: 38.0\n",
      "Episode: 48, Total reward: 38.0\n",
      "Episode: 49, Total reward: 47.0\n",
      "Episode: 50, Total reward: 34.0\n",
      "Episode: 51, Total reward: 31.0\n",
      "Episode: 52, Total reward: 55.0\n",
      "Episode: 53, Total reward: 60.0\n",
      "Episode: 54, Total reward: 39.0\n",
      "Episode: 55, Total reward: 36.0\n",
      "Episode: 56, Total reward: 26.0\n",
      "Episode: 57, Total reward: 40.0\n",
      "Episode: 58, Total reward: 51.0\n",
      "Episode: 59, Total reward: 24.0\n",
      "Episode: 60, Total reward: 44.0\n",
      "Episode: 61, Total reward: 38.0\n",
      "Episode: 62, Total reward: 34.0\n",
      "Episode: 63, Total reward: 25.0\n",
      "Episode: 64, Total reward: 51.0\n",
      "Episode: 65, Total reward: 25.0\n",
      "Episode: 66, Total reward: 39.0\n",
      "Episode: 67, Total reward: 46.0\n",
      "Episode: 68, Total reward: 26.0\n",
      "Episode: 69, Total reward: 49.0\n",
      "Episode: 70, Total reward: 47.0\n",
      "Episode: 71, Total reward: 38.0\n",
      "Episode: 72, Total reward: 39.0\n",
      "Episode: 73, Total reward: 47.0\n",
      "Episode: 74, Total reward: 51.0\n",
      "Episode: 75, Total reward: 24.0\n",
      "Episode: 76, Total reward: 44.0\n",
      "Episode: 77, Total reward: 56.0\n",
      "Episode: 78, Total reward: 34.0\n",
      "Episode: 79, Total reward: 40.0\n",
      "Episode: 80, Total reward: 40.0\n",
      "Episode: 81, Total reward: 53.0\n",
      "Episode: 82, Total reward: 41.0\n",
      "Episode: 83, Total reward: 49.0\n",
      "Episode: 84, Total reward: 47.0\n",
      "Episode: 85, Total reward: 39.0\n",
      "Episode: 86, Total reward: 52.0\n",
      "Episode: 87, Total reward: 39.0\n",
      "Episode: 88, Total reward: 57.0\n",
      "Episode: 89, Total reward: 56.0\n",
      "Episode: 90, Total reward: 58.0\n",
      "Episode: 91, Total reward: 49.0\n",
      "Episode: 92, Total reward: 38.0\n",
      "Episode: 93, Total reward: 36.0\n",
      "Episode: 94, Total reward: 47.0\n",
      "Episode: 95, Total reward: 56.0\n",
      "Episode: 96, Total reward: 62.0\n",
      "Episode: 97, Total reward: 37.0\n",
      "Episode: 98, Total reward: 41.0\n",
      "Episode: 99, Total reward: 36.0\n",
      "Episode: 100, Total reward: 52.0\n",
      "Episode: 101, Total reward: 49.0\n",
      "Episode: 102, Total reward: 38.0\n",
      "Episode: 103, Total reward: 34.0\n",
      "Episode: 104, Total reward: 37.0\n",
      "Episode: 105, Total reward: 26.0\n",
      "Episode: 106, Total reward: 38.0\n",
      "Episode: 107, Total reward: 25.0\n",
      "Episode: 108, Total reward: 25.0\n",
      "Episode: 109, Total reward: 50.0\n",
      "Episode: 110, Total reward: 40.0\n",
      "Episode: 111, Total reward: 53.0\n",
      "Episode: 112, Total reward: 37.0\n",
      "Episode: 113, Total reward: 55.0\n",
      "Episode: 114, Total reward: 34.0\n",
      "Episode: 115, Total reward: 41.0\n",
      "Episode: 116, Total reward: 51.0\n",
      "Episode: 117, Total reward: 47.0\n",
      "Episode: 118, Total reward: 44.0\n",
      "Episode: 119, Total reward: 56.0\n",
      "Episode: 120, Total reward: 31.0\n",
      "Episode: 121, Total reward: 40.0\n",
      "Episode: 122, Total reward: 39.0\n",
      "Episode: 123, Total reward: 47.0\n",
      "Episode: 124, Total reward: 25.0\n",
      "Episode: 125, Total reward: 45.0\n",
      "Episode: 126, Total reward: 51.0\n",
      "Episode: 127, Total reward: 48.0\n",
      "Episode: 128, Total reward: 44.0\n",
      "Episode: 129, Total reward: 38.0\n",
      "Episode: 130, Total reward: 38.0\n",
      "Episode: 131, Total reward: 36.0\n",
      "Episode: 132, Total reward: 27.0\n",
      "Episode: 133, Total reward: 40.0\n",
      "Episode: 134, Total reward: 42.0\n",
      "Episode: 135, Total reward: 44.0\n",
      "Episode: 136, Total reward: 52.0\n",
      "Episode: 137, Total reward: 45.0\n",
      "Episode: 138, Total reward: 50.0\n",
      "Episode: 139, Total reward: 50.0\n",
      "Episode: 140, Total reward: 32.0\n",
      "Episode: 141, Total reward: 52.0\n",
      "Episode: 142, Total reward: 31.0\n",
      "Episode: 143, Total reward: 39.0\n",
      "Episode: 144, Total reward: 38.0\n",
      "Episode: 145, Total reward: 35.0\n",
      "Episode: 146, Total reward: 35.0\n",
      "Episode: 147, Total reward: 41.0\n",
      "Episode: 148, Total reward: 53.0\n",
      "Episode: 149, Total reward: 35.0\n",
      "Episode: 150, Total reward: 52.0\n",
      "Episode: 151, Total reward: 40.0\n",
      "Episode: 152, Total reward: 46.0\n",
      "Episode: 153, Total reward: 25.0\n",
      "Episode: 154, Total reward: 47.0\n",
      "Episode: 155, Total reward: 35.0\n",
      "Episode: 156, Total reward: 52.0\n",
      "Episode: 157, Total reward: 25.0\n",
      "Episode: 158, Total reward: 35.0\n",
      "Episode: 159, Total reward: 58.0\n",
      "Episode: 160, Total reward: 41.0\n",
      "Episode: 161, Total reward: 39.0\n",
      "Episode: 162, Total reward: 39.0\n",
      "Episode: 163, Total reward: 34.0\n",
      "Episode: 164, Total reward: 39.0\n",
      "Episode: 165, Total reward: 39.0\n",
      "Episode: 166, Total reward: 42.0\n",
      "Episode: 167, Total reward: 34.0\n",
      "Episode: 168, Total reward: 42.0\n",
      "Episode: 169, Total reward: 38.0\n",
      "Episode: 170, Total reward: 52.0\n",
      "Episode: 171, Total reward: 25.0\n",
      "Episode: 172, Total reward: 52.0\n",
      "Episode: 173, Total reward: 50.0\n",
      "Episode: 174, Total reward: 40.0\n",
      "Episode: 175, Total reward: 42.0\n",
      "Episode: 176, Total reward: 25.0\n",
      "Episode: 177, Total reward: 59.0\n",
      "Episode: 178, Total reward: 47.0\n",
      "Episode: 179, Total reward: 25.0\n",
      "Episode: 180, Total reward: 41.0\n",
      "Episode: 181, Total reward: 42.0\n",
      "Episode: 182, Total reward: 39.0\n",
      "Episode: 183, Total reward: 41.0\n",
      "Episode: 184, Total reward: 60.0\n",
      "Episode: 185, Total reward: 47.0\n",
      "Episode: 186, Total reward: 35.0\n",
      "Episode: 187, Total reward: 26.0\n",
      "Episode: 188, Total reward: 47.0\n",
      "Episode: 189, Total reward: 37.0\n",
      "Episode: 190, Total reward: 38.0\n",
      "Episode: 191, Total reward: 43.0\n",
      "Episode: 192, Total reward: 37.0\n",
      "Episode: 193, Total reward: 35.0\n",
      "Episode: 194, Total reward: 34.0\n",
      "Episode: 195, Total reward: 25.0\n",
      "Episode: 196, Total reward: 56.0\n",
      "Episode: 197, Total reward: 39.0\n",
      "Episode: 198, Total reward: 52.0\n",
      "Episode: 199, Total reward: 41.0\n",
      "Episode: 200, Total reward: 40.0\n",
      "Episode: 201, Total reward: 37.0\n",
      "Episode: 202, Total reward: 53.0\n",
      "Episode: 203, Total reward: 40.0\n",
      "Episode: 204, Total reward: 47.0\n",
      "Episode: 205, Total reward: 50.0\n",
      "Episode: 206, Total reward: 44.0\n",
      "Episode: 207, Total reward: 47.0\n",
      "Episode: 208, Total reward: 35.0\n",
      "Episode: 209, Total reward: 35.0\n",
      "Episode: 210, Total reward: 39.0\n",
      "Episode: 211, Total reward: 52.0\n",
      "Episode: 212, Total reward: 36.0\n",
      "Episode: 213, Total reward: 39.0\n",
      "Episode: 214, Total reward: 51.0\n",
      "Episode: 215, Total reward: 52.0\n",
      "Episode: 216, Total reward: 39.0\n",
      "Episode: 217, Total reward: 31.0\n",
      "Episode: 218, Total reward: 30.0\n",
      "Episode: 219, Total reward: 39.0\n",
      "Episode: 220, Total reward: 38.0\n",
      "Episode: 221, Total reward: 39.0\n",
      "Episode: 222, Total reward: 49.0\n",
      "Episode: 223, Total reward: 41.0\n",
      "Episode: 224, Total reward: 47.0\n",
      "Episode: 225, Total reward: 37.0\n",
      "Episode: 226, Total reward: 26.0\n",
      "Episode: 227, Total reward: 39.0\n",
      "Episode: 228, Total reward: 38.0\n",
      "Episode: 229, Total reward: 35.0\n",
      "Episode: 230, Total reward: 58.0\n",
      "Episode: 231, Total reward: 56.0\n",
      "Episode: 232, Total reward: 36.0\n",
      "Episode: 233, Total reward: 38.0\n",
      "Episode: 234, Total reward: 48.0\n",
      "Episode: 235, Total reward: 37.0\n",
      "Episode: 236, Total reward: 26.0\n",
      "Episode: 237, Total reward: 34.0\n",
      "Episode: 238, Total reward: 46.0\n",
      "Episode: 239, Total reward: 57.0\n",
      "Episode: 240, Total reward: 56.0\n",
      "Episode: 241, Total reward: 38.0\n",
      "Episode: 242, Total reward: 53.0\n",
      "Episode: 243, Total reward: 39.0\n",
      "Episode: 244, Total reward: 34.0\n",
      "Episode: 245, Total reward: 25.0\n",
      "Episode: 246, Total reward: 58.0\n",
      "Episode: 247, Total reward: 55.0\n",
      "Episode: 248, Total reward: 64.0\n",
      "Episode: 249, Total reward: 40.0\n",
      "Episode: 250, Total reward: 36.0\n",
      "Episode: 251, Total reward: 38.0\n",
      "Episode: 252, Total reward: 39.0\n",
      "Episode: 253, Total reward: 25.0\n",
      "Episode: 254, Total reward: 44.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [9], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m state \u001b[38;5;241m=\u001b[39m state[\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m     12\u001b[0m action \u001b[38;5;241m=\u001b[39m control_theory(state) \u001b[38;5;66;03m# Get action\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m state, reward, done, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Take action\u001b[39;00m\n\u001b[1;32m     14\u001b[0m total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward \u001b[38;5;66;03m# Accrue reward\u001b[39;00m\n\u001b[1;32m     16\u001b[0m scores_deque\u001b[38;5;241m.\u001b[39mappend(total_reward)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/cartpole-cjUCt48w-py3.10/lib/python3.10/site-packages/gymnasium/wrappers/time_limit.py:50\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \n\u001b[1;32m     42\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     48\u001b[0m \n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/cartpole-cjUCt48w-py3.10/lib/python3.10/site-packages/gymnasium/wrappers/order_enforcing.py:37\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/cartpole-cjUCt48w-py3.10/lib/python3.10/site-packages/gymnasium/wrappers/env_checker.py:39\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/cartpole-cjUCt48w-py3.10/lib/python3.10/site-packages/gymnasium/envs/classic_control/cartpole.py:189\u001b[0m, in \u001b[0;36mCartPoleEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    186\u001b[0m     reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 189\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32), reward, terminated, \u001b[38;5;28;01mFalse\u001b[39;00m, {}\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/cartpole-cjUCt48w-py3.10/lib/python3.10/site-packages/gymnasium/envs/classic_control/cartpole.py:300\u001b[0m, in \u001b[0;36mCartPoleEnv.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    299\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mevent\u001b[38;5;241m.\u001b[39mpump()\n\u001b[0;32m--> 300\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrender_fps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    301\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mdisplay\u001b[38;5;241m.\u001b[39mflip()\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrgb_array\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Play the game many times\n",
    "scores_deque = deque(maxlen=100)\n",
    "scores = []\n",
    "for episode in range(0, 500): # 500 episodes of learning\n",
    "    total_reward = 0 # Maintains the score for this episode.\n",
    "    state = env.reset()\n",
    "\n",
    "    while True:\n",
    "        if isinstance(state, tuple):\n",
    "            state = state[0]\n",
    "        state = state[2]\n",
    "        action = control_theory(state) # Get action\n",
    "        state, reward, done, truncated, info = env.step(action) # Take action\n",
    "        total_reward += reward # Accrue reward\n",
    "                \n",
    "        scores_deque.append(total_reward)\n",
    "        scores.append(total_reward)\n",
    "\n",
    "        if done or truncated: # Episode is completed due to failure or cap being reached.\n",
    "            print(\"Episode: {}, Total reward: {}\".format(episode, total_reward))\n",
    "            break\n",
    "\n",
    "    if np.mean(scores_deque)>=threshold:\n",
    "        print('Environment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78771af6-5c11-4d9e-8cb0-ff451e32682c",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94240e8c-41b7-44cd-a26a-97d2a732fed2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
